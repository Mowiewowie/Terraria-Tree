import json
import time
import urllib.parse
import requests
from bs4 import BeautifulSoup

# Define the target file generated by the C# script
JSON_PATH = 'Terraria_Vanilla_1.4.4_Export.json' # Adjust this to match your exact output filename
WIKI_BASE = 'https://terraria.wiki.gg/wiki/'

def sanitize_text(text):
    """Strips invisible characters and malicious HTML entities."""
    if not text: return ""
    return text.replace('\xa0', ' ').strip()

def scrape_acquisition_data(item_name):
    """Scrapes the wiki for NPC shop and Chest/Crate locations."""
    safe_name = urllib.parse.quote(item_name.replace(" ", "_"))
    url = f"{WIKI_BASE}{safe_name}"
    
    # SECURITY: Hard timeout to prevent CI/CD hanging, and a custom User-Agent to avoid generic bot blocks
    headers = {'User-Agent': 'TerrariTree-Data-Builder/1.0 (https://github.com/yourusername/yourrepo)'}
    
    try:
        response = requests.get(url, headers=headers, timeout=10)
        if response.status_code != 200:
            return []
            
        soup = BeautifulSoup(response.text, 'html.parser')
        new_drops = []
        
        # 1. Check for "Sold by" tables (NPC Shops)
        sold_by_span = soup.find('span', id='Sold_by')
        if sold_by_span:
            table = sold_by_span.find_next('table', class_='cargoTable')
            if table:
                rows = table.find_all('tr')[1:] # Skip header
                for row in rows:
                    cols = row.find_all('td')
                    if len(cols) >= 3:
                        npc_name = sanitize_text(cols[0].get_text())
                        conditions = sanitize_text(cols[2].get_text())
                        new_drops.append({
                            "SourceNPC_ID": -1,
                            "SourceNPC_Name": f"NPC: {npc_name}",
                            "DropChance": "100%", # Shops are guaranteed if conditions are met
                            "Conditions": [conditions] if conditions else []
                        })

        # 2. Check for "Found in" tables (Chests, Crates)
        found_in_span = soup.find('span', id='Found_in')
        if found_in_span:
            table = found_in_span.find_next('table', class_='cargoTable')
            if table:
                rows = table.find_all('tr')[1:]
                for row in rows:
                    cols = row.find_all('td')
                    if len(cols) >= 3:
                        container_name = sanitize_text(cols[0].get_text())
                        chance = sanitize_text(cols[2].get_text())
                        new_drops.append({
                            "SourceNPC_ID": -1,
                            "SourceNPC_Name": f"Chest/Crate: {container_name}",
                            "DropChance": chance,
                            "Conditions": []
                        })
                        
        return new_drops
        
    except requests.exceptions.RequestException as e:
        print(f"[Warning] Failed to fetch {item_name}: {e}")
        return []

def main():
    print(f"Loading {JSON_PATH}...")
    try:
        with open(JSON_PATH, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except FileNotFoundError:
        print(f"Error: {JSON_PATH} not found. Ensure C# script runs first.")
        return

    updated_count = 0

    for item in data:
        # Only query items that have absolutely no source of acquisition
        has_recipes = len(item.get("Recipes", [])) > 0
        has_drops = len(item.get("ObtainedFromDrops", [])) > 0
        
        if not has_recipes and not has_drops:
            print(f"Scraping missing data for: {item['DisplayName']}...")
            new_acquisition = scrape_acquisition_data(item['DisplayName'])
            
            if new_acquisition:
                item["ObtainedFromDrops"].extend(new_acquisition)
                updated_count += 1
                
            time.sleep(1) # Polite delay to respect wiki.gg rate limits

    print(f"Finished injecting wiki data. Updated {updated_count} items.")
    
    with open(JSON_PATH, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=4)

if __name__ == "__main__":
    main()